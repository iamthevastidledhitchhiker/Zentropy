import sys
sys.path.append("components/summarizer/pointer-generator")
import components.summarizer.summarizer_utils as sutils
import components.summarizer.story_converter as sconv
import pickle
import nltk.tokenize as tokenize
import os
from nltk.tokenize.moses import MosesDetokenizer


# Define which articles you want to summarize:
articles = [
    "http://www.bbc.com/news/business-43967923",
    "https://www.theguardian.com/technology/2018/may/02/tesla-loss-model-3-elon-musk",
    "https://www.theguardian.com/world/2018/may/03/japan-robot-dogs-get-solemn-buddhist-send-off-at-funerals"
]

# Fetch the articles from the internet, and store them in a pickle:
print("Downloading articles...")
story_data = sutils.fetch_and_pickle_stories(articles, 'data/pickles/raw_stories.pickle', 'data/stories/', False)
print("Downloading articles DONE")

print("-"*100)

# Conver the articles into a format that a the summarizer model can consume
print("Converting articles into binary format for summarization model...")
sconv.process_and_save_to_disk(story_data['stories'], "test.bin", "data/converted_articles")
print("Converting articles into binary format for summarization model DONE")

# Run summarizer model (uncomment different exp_names to run with different models):
DATA_DIR = 'data/'
summarizer_internal_pickle = f"{DATA_DIR}pickles/decoded_stories.pickle"
data_path = f"{DATA_DIR}converted_articles/chunked/test_*"
vocab_path = f"{DATA_DIR}summarizer_training_data/finished_files/vocab"
log_root = f"{DATA_DIR}summarizer_models"
exp_name = "more_coverage"
#exp_name = "no_coverage"
#exp_name = "some_coverage"

sutils.run_summarization_model_decoder(summarizer_internal_pickle, data_path = data_path,
           vocab_path = vocab_path, log_root = log_root, exp_name = exp_name)


# Load the results of the summarizer model output:
summarization_output = pickle.load(open(summarizer_internal_pickle, "rb" ))

print("-"*100)
print("-"*100)
print("-"*100)

print("Summaries generated by the neural summarizer - case insensitive:")
for s in summarization_output['summaries']:
    print(s+"\n\n")

print("-"*100)

# Attempt to fix lower case to correct upper case:
tokenized_summaries = sutils.try_fix_upper_case_for_summaries(story_data['stories'], summarization_output['summaries_tokens'])

detokenizer = MosesDetokenizer()

detokenized_summaries = []

print("Summaries generated by the neural summarizer - casing fixed:")
for s in tokenized_summaries:
    s_detok = detokenizer.detokenize(s, return_str=True)
    detokenized_summaries.append(s_detok)
    print(s_detok+"\n\n")
    
print("-"*100)

print("Look at baseline summaries")

print("-"*100)
print("Extractive summaries:\n")
print("-"*100)
for s1 in story_data['summaries_extractive']:
    print(s1+"\n\n")
print("-"*100)
print("3 sentence summaries:\n")
print("-"*100)
for s2 in story_data['summaries_3sent']:
    print(s2+"\n\n")

print("-"*100)
print("Load NER model")
import components.ner.NERutils as ner
print("-"*100)
all_orgs = []

# Run NER model:
for story in detokenized_summaries:
    storyCombined = story.replace('\n', ' ')

    print('RUNNING TOKENIZER')
    storyTokenized = tokenize.word_tokenize(storyCombined)

    print('SPLITTING SENTENCES LINE BY LINE')
    split = ner.sentenceSplitter(storyTokenized)

    inputFile = open(r'components/ner/input.txt','w')
    ner.writeArticle(split,inputFile)
    inputFile.close()

    print('RUNNING MODEL')
    os.system('python2.7 components/ner/tagger-master/tagger.py --model components/ner/tagger-master/models/english/ --input components/ner/input.txt --output components/ner/output.txt')

    with open(r'components/ner/output.txt','r') as namedStory:
        namedStory=namedStory.read().replace('\n', ' ')

    print('NAMED ENTITIES:')
    orgs  = ner.findNamedEntities(namedStory.split(' '))
    all_orgs.append(orgs)
    print(orgs)
    
print("\n\n")
print("-"*100)
print("Results:")

for i, url in enumerate(articles):
    print("-"*100)
    print(f"{url}\n")
    print(f"Neural summary: {detokenized_summaries[i]}\n")
    print(f"3 sentence summary: {story_data['summaries_3sent'][i]}\n")
    print(f"Organizations: {all_orgs[i]}")
    print("-"*100)
print("-"*100)
